# 方針
## 1. グリッパー作成
1. 基本的にはこの論文を参考に作成
	1. [[GraspEveryThing_2025.pdf]]
	2. [[PolyTouch_2025.pdf]]
3. Aloha handを利用
4. 必要なもの
	- [ ] Finger(UMI-Gripperを参考に, [[2025.10.31]]まで)
	- [x] M3 Clear テープ
	- [x] WS2812 cob ledX3
	- [x] アルミパウダー(反射用)
	- [x] グリップテープ
	- [ ] グリッパー([[2025.10.31]]まで)
5. 複数LEDストリッパーのライティング
-  
## 2. RPiからPCへのストリーミング
1. [ストリーミングドライバ](https://github.com/GelSight-lab/GraspEveryThing/tree/main/software)
	* 基本的には前回やったストリーミングと同じでHTTPに画像転送のはず
	* mjpegとflaskを使用
	* run.py->ros2-topicとして保存
## 3. 深層学習モデル
1. 画像からの特徴抽出にはT3というモデルを使用
2. フィンガーからの画像+トルクセンサー+(可能ならグリッパーの開閉距離を合わせて学習)
## 4. Diffusion Policy
1. 
## 5. 最終的な実験
1. 自作のグリッパーなのでT3を使った識別試験をやってみるのもいいかも
2. 柔らかい食品
3. 風船にコーヒー殻を入れたもの
4. マシュマロ
5. 比較的大きなものを実験に利用したい
6. グリッパーのみの検証

